{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stock-population",
   "metadata": {},
   "source": [
    "# 3. 특징생성\n",
    "# 3.12.5. 자연어 처리 기법\n",
    "- 어간추출, 불용어, 표제어 추출 등 자연어 처리 특유의 전처리는 설명하지 않습니다.\n",
    "\n",
    "## bag-of-words(BoW)\n",
    "- 문장 등의 텍스트를 단어로 분할하고, 각 단어의 출현 수를 **순서에 상관없이 단순히 세는 방법**입니다. \n",
    "- Bag of Words는 직역하면 '단어들의 가방' 입니다. Bag of Words는 문서를 단어들의 가방으로 가정합니다.\n",
    "- Bag of Words 는 자연어 처리(Natural Language Processing)이나 정보 검색(Information Retrieval)에서 쓰이는 매우 간단한 단어 표현 방법으로, 문서 내의 단어들의 분포를 보고 이 문서의 특성을 파악하는 기법입니다. 줄여서 BoW라고 표기하기도 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qualified-sheriff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words: [[1 1 1 1 3 2 2 1 2 2]]\n",
      "각 단어의 인덱스: {'avocado': 1, 'likes': 4, 'to': 6, 'watch': 9, 'movies': 5, 'walnut': 8, 'too': 7, 'also': 0, 'football': 2, 'games': 3}\n",
      "단어장(Vocabulary)의 크기: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence = ['Avocado likes to watch movies. Walnut likes movies too! Walnut also likes to watch football games.']\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(sentence).toarray()\n",
    "\n",
    "print('Bag of Words:', bow) #- 코퍼스로부터 각 단어의 빈도수를 기록합니다.\n",
    "print('각 단어의 인덱스:', vector.vocabulary_) #- 각 단어의 인덱스가 어떻게 부여되었는지를 보여줍니다. \n",
    "print('단어장(Vocabulary)의 크기:', len(vector.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-consultancy",
   "metadata": {},
   "source": [
    "# n-gram\n",
    "- BoW에서 분할하는 단위를, 단어가 아닌 연속되는 단어 뭉치로 끊는 방법입니다. \n",
    "- 단어분할에 비해 텍스트에 포함된 정보를 유지하기는 좋지만, 출현 가능한 종류의 수가 크게 늘어나며, 희소데이터가 된다는 문제가 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "three-fancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avocado',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'movies.',\n",
       " 'Walnut',\n",
       " 'likes',\n",
       " 'movies',\n",
       " 'too!',\n",
       " 'Walnut',\n",
       " 'also',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'football',\n",
       " 'games.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentence[0].split()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "following-moldova",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Avocado', 'likes'),\n",
       " ('likes', 'to'),\n",
       " ('to', 'watch'),\n",
       " ('watch', 'movies.'),\n",
       " ('movies.', 'Walnut'),\n",
       " ('Walnut', 'likes'),\n",
       " ('likes', 'movies'),\n",
       " ('movies', 'too!'),\n",
       " ('too!', 'Walnut'),\n",
       " ('Walnut', 'also'),\n",
       " ('also', 'likes'),\n",
       " ('likes', 'to'),\n",
       " ('to', 'watch'),\n",
       " ('watch', 'football'),\n",
       " ('football', 'games.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*[sentence[i:] for i in range(2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-timeline",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "- TF-IDF는 각 단어의 중요도를 판단하여 가중치는 주는 방법입니다. \n",
    "- TF-IDF는 **모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다.** \n",
    "- TF-IDF는 Term Frequency-Inverse Document Frequency의 약자입니다. 한국어로 해석하면 '단어 빈도-역문서 빈도'입니다. '단어의 빈도'와 '문서의 빈도의 역수'를 활용하는 것입니다. \n",
    "- TF-IDF는 불용어처럼 중요도가 낮으면서 모든 문서에 등장하는 단어들이 노이즈가 되는 것을 완화해줍니다. \n",
    "- 그러나 TF-IDF를 사용하는 것이 DTM을 사용하는 것보다 성능이 항상 뛰어나지는 않습니다. \n",
    "- TF-IDF를 사용하기 위해서는 우선 DTM을 만든 뒤 TF-IDF가중치를 DTM에 적용합니다. \n",
    "- DTM은 이미 TF(Term Frequency, 단어의 빈도)행렬이기 떄문에, 각 단어에 IDF값을 곱해주면 TF-IDF행렬이 완성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "religious-collectible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>avocado</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>likes</th>\n",
       "      <th>movies</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>walnut</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336315</td>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373119</td>\n",
       "      <td>0.480458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631745</td>\n",
       "      <td>0.480458</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.337295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       also   avocado  football     games     likes    movies        to  \\\n",
       "0  0.000000  0.569431  0.000000  0.000000  0.336315  0.433067  0.433067   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.373119  0.480458  0.000000   \n",
       "2  0.443503  0.000000  0.443503  0.443503  0.261940  0.000000  0.337295   \n",
       "\n",
       "        too    walnut     watch  \n",
       "0  0.000000  0.000000  0.433067  \n",
       "1  0.631745  0.480458  0.000000  \n",
       "2  0.000000  0.337295  0.337295  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'Avocado likes to watch movies.',\n",
    "    'Walnut likes movies too!',\n",
    "    'Walnut also likes to watch football games.'\n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "vocab = list(tfidfv.vocabulary_.keys()) #- 단어장을 리스트로 저장\n",
    "vocab.sort() #- 단어장을 알파벳 순으로 정렬\n",
    "\n",
    "#- TF-IDF 행렬에 단어장을 데이터프레임의 열로 지정하여 데이터프레임 생성\n",
    "tfidf_ = pd.DataFrame(tfidfv.transform(corpus).toarray(), columns=vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-instrumentation",
   "metadata": {},
   "source": [
    "## 단어 임베딩\n",
    "- 단어를 수치 벡터로 변환하는 방법을 단어 임베딩이라고 합니다. 학습이 완료된 임베딩을 사용하면 단어를 그들의 의미와 성질이 반영된 수치 벡터로 변환할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-hamilton",
   "metadata": {},
   "source": [
    "# 3.12.6. 자연어 처리 기법의 응용\n",
    "- BoW나 n-gram, tf-idf와 같이 자연어 처리에서 많이 사용되는 기법은 실제 자연어와 관계 없는 데이터에도 적용할 수 있습니다. \n",
    "- 예를 들면 어떠한 단어 등 요소 배열이 있을 때, 이를 문장처럼 파악하고 자연어 처리 방법을 적용할 수 있습니다. \n",
    "- 예를 들어 캐글의 [Walmart Recruting : Trip Type Classification](https://www.kaggle.com/c/walmart-recruiting-trip-type-classification/data) 대회는 각 길이가 다른 사용자의 구매 품목 배열을 주고, 그것을 분류하는 문제였습니다. \n",
    "    - 이 문제에서는 해당 배열을 문장으로 보고, 구매 품목을 단어로서 카운트 행렬로 만드는 것이 기본적 방법으로 여겨졌습니다.\n",
    "    - ['toothbrush','body wash','shampoo','toothbrush',...]\n",
    "- 캐글 [Microsoft Malware Classification Challenge](https://www.kaggle.com/c/malware-classification) 대회에서는 바이너리 파일 내용을 16진수 형식으로 표현하는 헥사 덤프와 어셈블리 코드가 주어졌습니다. 헥사 덤프의 1바이트를 한 단어로 간주하고 단어나 n-gram을 구해서 카운트하는 방식이 사용되었습니다. 또한 1행을 한 단어로 간주하는 방법도 있었습니다. \n",
    "    - hex dump : 램 또는 파일이나 저장장치에 있는 컴퓨터 데이터의 십육진법적인 보임새입니다. 데이터의 hex dump를 보는 것은 주로 디버깅이나 리버스 엔지니어링의 한 부분입니다.\n",
    "   - 참고 : http://bahndal.egloos.com/569680\n",
    "\n",
    "- 캐글 Otto Group Product Classification Challenge에서는 특징의 의미는 부여되지 않았지만 값이 모두 0 이상이었던 만큼, 주어진 특징을 '무언가의 출현 횟수를 계산한 행렬'로 간주하고 tf-idf를 구했습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-citation",
   "metadata": {},
   "source": [
    "# 3.12.7. 토픽 모델을 활용한 범주형 변수 변환\n",
    "- 토픽 모델이라는 문서 분류 기법을 응용하여, 다른 범주형 변수와의 동시출현 정보로부터 범주형 변수를 수치 벡터로 변환하는 방법이 있습니다.\n",
    "    - 토픽 모델링은 텍스트 데이터에서 사용된 주제어들의 동시 사용 패턴을 바탕으로, 해당 텍스트들을 대표하는 특정 주제나 이슈, 주제 그룹들을 자동으로 추출하는 분석 기법입니다.\n",
    "    - 범주형 변수란 고유한 값이나 범주 수가 제한된 변수(예: 성별 또는 종교)입니다.\n",
    "- 두 범주형 변수 중 하나를 문서, 다른 하나를 단어로 간주하면, 각 문서에 각 단어가 몇 번 등장하는지 동시출현 정보를 통해 단어-문서 카운트 행렬을 만들 수 있습니다. 여기에 LDA(Latent Dirichlet Allocation)를 적용하면 첫번째 변수를 문서가 속한 토픽에 대한 확률을 나타내는 수치 벡터로 변환할 수 있습니다. \n",
    "    - 첫번째 변수 ; 문서 ; 집 포함 옵션 '화장실','거실','부엌','복도','화장실','화장실',..\n",
    "    - 두번째 변수 ; 단어 ; 집 구매시 '화장실' 또는 '거실' 중 중요한 요소\n",
    "- 캐글 TalkingData AdTracking Fraud Detection Challenge 대회의 1위 솔루션이 이 방법을 사용했습니다. \n",
    "- 이 대회는 사용자가 모바일앱 광고를 클릭한 후 실제로 내려받을지 여부를 예측하는 문제로, 주어진 데이터로는 사용자(IP주소), 광고 대상 앱, 접속 단말 등의 범주형 변수가 있습니다. \n",
    "- 예를 들어 사용자를 문서, 앱을 단어로 간주하고 이 기법을 적용하면, 어떤 앱의 광고를 클릭할지에 관한 경향에 근거하여 분류된 여러 그룹에 사용자가 속할 확률이 계산됩니다. 이 특징은 사용자가 내려받을 만한 앱을 추측할 때 효과적입니다. \n",
    "- 반면 LDA 외에 PCA와 NMF도 사용했는데, 그 중 LDA가 가장 효과적이었다고 합니다. 이 대회의 1위 팀은 접속 단말을 제외한 4개의 범주형 변수의 모든 조합(4x3=12)에 대해 토픽 수 5로 LDA를 적용하여 총 60개의 특징을 생성했습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-reducing",
   "metadata": {},
   "source": [
    "# 3.12.8. 이미지 특징을 다루는 방법\n",
    "- 이미지 데이터를 특징으로 만들 때는 이미지넷 데이터로 학습한 신경망으로 이미지를 예측하고, 출력층에 가까운 층의 출력값을 특징으로 하는 방법을 사용합니다. \n",
    "- 또한 이미지 크기나 색상, 밝기와 같은 기본적인 이미지의 특징, 딥러닝 이전에 많이 쓰이던 SIFT 등의 특징, EXIF 태크와 같은 메타 정보도 활용할 수 있습니다. \n",
    "    - SIFT (Scale-Invariant Feature Transform)은 이미지의 크기와 회전에 불변하는 특징을 추출하는 알고리즘입니다.\n",
    "    - EXIF stands for \"Exchangeable Image File Format\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-piano",
   "metadata": {},
   "source": [
    "# 3.12.9. 결정 트리의 특징 변환 \n",
    "- 결정트리를 작성한 뒤, 각 행 데이터가 분기에 따라 어느 잎 노드로 떨어지는지를 범주형 변수의 특징으로 만들어 다른 모델에 투입하는 독특한 기술입니다.\n",
    "- GBDT 모델에서 만들어진 일련의 결정 트리에 많이 쓰입니다. \n",
    "- 캐글 Display Advertising Challenge 대회와 Click-Through Rate Prediction 대회의 1위 솔루션에서 이방법으로 생성한 특징을 FFM(field- aware factorization machine)이라는 모델에 도입했습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-maria",
   "metadata": {},
   "source": [
    "# 3.12.10. 익명화 데이터의 원래 값 추측\n",
    "- 경진대회에서는 주최자의 의향에 따라 각 변수의 의미가 숨겨져 있거나 또는 값에 표준화와 같은 처리를 가한 상태의 데이터를 제공할 때가 있습니다. \n",
    "- 데이터를 주의 깊에 관찰하면 변환 전 원래 값으로 되돌릴 수 있기도 합니다. \n",
    "- 예를 들어 '나이'가 표준화 되어 주어졌을 때, 값의 간격으로부터 이산값(discrete value)를 대략 알 수 있으므로, 먼저 값 간격 중의 오차를 제외하고, 가장 작은 값으로 나눗셈을 해봅니다. 그러면 거의 정수가 되므로, 최댓값/최솟값이나 분포, 많이 출현하는 값 등을 바탕으로 해당 값이 나이인지 아닌지를 추측할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-interface",
   "metadata": {},
   "source": [
    "# 3.12.11 데이터 오류 수정\n",
    "- 일부 데이터가 사용자나 데이터 작성자의 오류로 인해 잘못되었다고 추측될 경우, 수정을 거치면서 품질이 더 좋은 데이터로 학습시킬 수 있습니다. \n",
    "- 캐글의 Airbnb New User Booking 대회에서는 나이를 나타내는 변수에 출생년도가 입력되어 있거나, 100세 이상의 부자연스러운 나이가 입력되어 있었으므로, 값을 수정하여 모델 성능을 높일 수 있었습니다. 한편 자연어 처리 문제에서는 전처리의 하나로서 스펠링 오류 수정이 자주 이뤄집니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-liechtenstein",
   "metadata": {},
   "source": [
    "# 3.13. 경진대회의 특징 사례\n",
    "- 다양한 캐글 경진대회에서 생성된 특징을 살펴봅니다. \n",
    "- 상상력이 요구됨\n",
    "# 3.13.1 Recruit Restaurant Visitor Forecating\n",
    "- 다수 음식점의 미래 방문객 수 예측\n",
    "- 학습 데이터 : 16/1/1~17/4/22까지의 음식점별 방문객 수와 예약 건수\n",
    "- 목표 : 17/4/23~17/5/31까지의 방문객 수 예측\n",
    "---\n",
    "\n",
    "\n",
    "1. 예측 대상일로부터 7일 간격으로 50주간 일별 방문자 수의 로그\n",
    "    - 음식점의 손님 수는 요일에 따라 크게 달라지기 때문에, 같은 요일의 과거 실적을 lag 특징으로 추가함. \n",
    "2. 앞의 방문자 수 로그로부터 만들어진 특징을 활용한 8주간의 평균\n",
    "    - 같은 요일이라도 날짜에 따른 편차가 존재하기 때문에, 평균화\n",
    "3. 이용 가능한 최신 날짜로부터 20일간 방문자 수의 로그\n",
    "4. 이용 가능한 최신 날짜로부터 5주간 주별 방문자 수의 로그 평균\n",
    "5. 예측 대상일을 포함한 과거 7일간 일별 예약 건수의 로그 \n",
    "\n",
    "- 로그화 후 평균을 구한 이유는, 공휴일 등 예외적으로 방문자가 많은 날의 영향을 완화하려는 것\n",
    "- 방문자 수가 0인 데이터가 있으므로 log1p 사용\n",
    "- 골든 위크가 시작되기 직전의 요일은 금요일로 가정하여 특징을 만듦\n",
    "\n",
    "https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-object",
   "metadata": {},
   "source": [
    "# 3.13.2 Standard Product Recommendation\n",
    "- 산탄데르 은행의 **각 사용자별 금융상품 구매 이력이 월 단위로 주어지고, 이를 바탕으로 최신 월의 구매 상품을 사용자 별로 예측**\n",
    "- 학습 데이터 : 금융 상품은 총 24종류이며 15년 2월 ~ 16년 5월 까지의 상품별 구매 실적\n",
    "- 목표 : 16년 6월 구매 상품 예측(단, 전월인 5월에는 구매하지 않았으나, 6월에 구매한 상품만 예측). 가장 구매 가능성이 높은 상품부터 상위 7개 목록 제출\n",
    "\n",
    "1. 전월의 각 상품별 구매 여부 플래그\n",
    "    - lag 특징\n",
    "2. 해당 플래그를 모두 연결한 문자열\n",
    "    - 범주형 변수, 타깃 인코딩\n",
    "3. 각 상품의 구매 여부 플래스가 0->0, 0->1, 1->0, 1->1 으로 전이된 횟수\n",
    "    - 구매 여부의 값의 변경 확인\n",
    "    - 새로운 고객유입 확인\n",
    "    - **우리는 전원에 구매하지 않았던 사용자의 이번달 신규 구매 여부를 예측해야 하므로, 0->1로 전이되는 확률이 중요합니다.**\n",
    "4. 각 상품을 구매하지 않은 상태의 유지 기간(월 단위)\n",
    "    - 구매하지 않은 상태가 이어질수록 다음에도 구매하기 어려워지거나 또는 구매하기 쉬워지는 경향이 있을것이라 가정\n",
    "5. 전월 구매한 상품 수\n",
    "    - 4번 가정과 동일\n",
    "\n",
    "https://www.kaggle.com/c/santander-product-recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-scholarship",
   "metadata": {},
   "source": [
    "# 3.13.3 Instacart Market Basket Analysis\n",
    "- **과거의 주문 내역을 바탕으로 다음 주문 시 재주문할 구매 상품을 예측**하는 문제(재주문한 상품이 없을 경우 None으로 예측하여 제출)\n",
    "\n",
    "1. 사용자 기반 특징\n",
    "    - 재주문 빈도\n",
    "    - 주문 간격\n",
    "    - 주문 시간대\n",
    "    - 유기농, 글루텐프리, 아시아 아이템의 과거 주문 여부\n",
    "    - 1회 주문 시 상품 수의 특징\n",
    "    - 처음 구매하는 아이템을 포함하는 주문량\n",
    "2. 아이템 기반 특징\n",
    "    - 구매 빈도\n",
    "    - 장바구니에서의 위치\n",
    "    - 일회성 구매에 그칠 확률\n",
    "    - 동시 구매 상품 수의 통계량\n",
    "    - 같은 카테고리 제품을 구매한 후 다른 제품의 추가 구매 여부에 관한 통계량(예: 지난 번 바나나 샀을 경우, 다음엔 딸기를 구매할지 여부)\n",
    "    - 끝임 없이 이어지는 연속 주문에 관한 통계량\n",
    "    - N회 주문 중 재주문될 확률\n",
    "    - 주문 발생 요일의 분포\n",
    "    - 최초 주문 후 재주문될 가능성\n",
    "    - 주문 간격 통계량\n",
    "3. 사용자 x 아이템 기반 특징\n",
    "    - 사용자가 해당 상품을 구매한 횟수\n",
    "    - 사용자가 해당 상품을 마지막으로 구매한 이후 경과시간\n",
    "    - 연속 주문\n",
    "    - 장바구니에서의 위치\n",
    "    - 당일에 해당 사용자가 이미 그 아이템을 주문했는지 여부\n",
    "    - 동시 구매되는 상품에 관한 통계량\n",
    "    - 어떤 상품 대신 구매되는 상품의 통게량 ( = 주문 이후 추가로 연속 구매되지 않은 상품)\n",
    "   \n",
    "4. 날짜 기반 특징\n",
    "    - 요일별 주문 건수, 주문된 상품 개수\n",
    "    - 시간당 주문 건수, 주문된 상품 개수\n",
    "\n",
    "https://www.kaggle.com/c/instacart-market-basket-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-wisconsin",
   "metadata": {},
   "source": [
    "# 3.13.4 KDD Cup 2015\n",
    "- **중국의 온라인 공개수업(MOOC)사이트에서 수강자의 이탈을 예측하는 것**\n",
    "- 접속 로그가 부여되고, 그것을 기반으로 이수등록 및 사용자별 이탈여부를 예측합니다.\n",
    "- 접속 로그를 그대로 모델의 입력으로 삼을 수 없기에, 집약하여 특징을 생성합니다.\n",
    "\n",
    "1. 데이터 구조를 기점으로 특징 생성\n",
    "    - 대상 범위 : 모든 기간, 특정 요일, 시간대 및 기간, 특정 이벤트 등\n",
    "    - 집약 방법 : 이수 등록 단위, 사용자 단위, 코스 단위 등\n",
    "    - 지표 : 로그 건수, 접속 일수 등\n",
    "    - 위 항목들을 조합하여, '이수등록 정보별 주말 접속 일수', '사용자별 수업과제 중임을 나타내는 값의 로그 건수'와 같은 다양한 특징을 추출할 수 있습니다.\n",
    "  \n",
    "2. 사용자 행동을 기점으로 특징 생성\n",
    "    - 사용자의 성실도\n",
    "        - 방문 일수나 동양상 시청 건수 등 사용자의 성실도를 나타내는 특징\n",
    "    - 사용자의 학습 진척도\n",
    "        - 접속 로그로부터 사용자의 수업 진척도와 평균적인 진척도와의 차이를 산출하여 만들어내는 특징\n",
    "    - 미래의 로그\n",
    "        - 사용자 활동이 관측되지 않는 미래 기간에, 사용자가 다른 코스에서 활동할지 여부를 관측하여 알아낸 정보로 만드는 특징(실무에서는 불가능..)\n",
    "https://www.biendata.xyz/competition/kddcup2015/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
